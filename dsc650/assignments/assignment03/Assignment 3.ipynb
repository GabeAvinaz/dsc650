{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries and define common helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import json\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import pyarrow as pa\n",
    "#  from pyarrow.json import read_json\n",
    "import pyarrow.parquet as pq\n",
    "import fastavro\n",
    "import pygeohash\n",
    "import snappy\n",
    "import jsonschema\n",
    "from jsonschema.exceptions import ValidationError\n",
    "\n",
    "\n",
    "endpoint_url='https://storage.budsc.midwest-datascience.com'\n",
    "\n",
    "current_dir = Path(os.getcwd()).absolute()\n",
    "schema_dir = current_dir.joinpath('schemas')\n",
    "results_dir = current_dir.joinpath('results')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def read_jsonl_data():\n",
    "    # s3 = s3fs.S3FileSystem(\n",
    "    #     anon=True,\n",
    "    #     client_kwargs={\n",
    "    #         'endpoint_url': endpoint_url\n",
    "    #     }\n",
    "    # )\n",
    "    # src_data_path = 'data/processed/openflights/routes.jsonl.gz'\n",
    "    # with s3.open(src_data_path, 'rb') as f_gz:\n",
    "    #     with gzip.open(f_gz, 'rb') as f:\n",
    "    #         records = [json.loads(line) for line in f.readlines()]\n",
    "\n",
    "    src_data_path = '../../../data/processed/openflights/routes.jsonl.gz'\n",
    "    with gzip.open(src_data_path, 'rb') as f:\n",
    "        records = [json.loads(line) for line in f.readlines()]\n",
    "        \n",
    "\n",
    "    return records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the records from https://storage.budsc.midwest-datascience.com/data/processed/openflights/routes.jsonl.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = read_jsonl_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.a JSON Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def validate_jsonl_data(records):\n",
    "    schema_path = schema_dir.joinpath('routes-schema.json')\n",
    "    with open(schema_path) as f:\n",
    "        # schema = jsonschema.Draft7Validator(records[0]).schema\n",
    "        # f.write(json.dumps(schema, indent=4))\n",
    "        schema = json.load(f)\n",
    "    validation_csv_path = ('validation.md')\n",
    "    with open(validation_csv_path, 'w') as f:    \n",
    "        for i, record in enumerate(records):\n",
    "            try:\n",
    "                jsonschema.validate(record, schema)\n",
    "            except ValidationError as e:\n",
    "                f.write(f\"Failed Entry: {i}\\n\")\n",
    "            \n",
    "\n",
    "validate_jsonl_data(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.b Avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "SchemaParseException",
     "evalue": "Default value <NONE> must match schema type: record",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSchemaParseException\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 19\u001b[0m\n\u001b[0;32m     11\u001b[0m         fastavro\u001b[39m.\u001b[39mwriter(out, parsed_schema, records)\n\u001b[0;32m     13\u001b[0m     \u001b[39m# used to test file output\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[39m# with open(data_path, 'rb') as fo:\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[39m#     avro_reader = fastavro.reader(fo)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[39m#     for record in avro_reader:\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[39m#         print(record)\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m create_avro_dataset(records)\n",
      "Cell \u001b[1;32mIn[66], line 11\u001b[0m, in \u001b[0;36mcreate_avro_dataset\u001b[1;34m(records)\u001b[0m\n\u001b[0;32m      8\u001b[0m     parsed_schema \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(f)\n\u001b[0;32m     10\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(data_path, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m out:\n\u001b[1;32m---> 11\u001b[0m     fastavro\u001b[39m.\u001b[39;49mwriter(out, parsed_schema, records)\n",
      "File \u001b[1;32mfastavro/_write.pyx:762\u001b[0m, in \u001b[0;36mfastavro._write.writer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mfastavro/_write.pyx:670\u001b[0m, in \u001b[0;36mfastavro._write.Writer.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mfastavro/_schema.pyx:146\u001b[0m, in \u001b[0;36mfastavro._schema.parse_schema\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mfastavro/_schema.pyx:381\u001b[0m, in \u001b[0;36mfastavro._schema._parse_schema\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mfastavro/_schema.pyx:449\u001b[0m, in \u001b[0;36mfastavro._schema.parse_field\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mfastavro/_schema.pyx:372\u001b[0m, in \u001b[0;36mfastavro._schema._parse_schema\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mfastavro/_schema.pyx:168\u001b[0m, in \u001b[0;36mfastavro._schema._raise_default_value_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mSchemaParseException\u001b[0m: Default value <NONE> must match schema type: record"
     ]
    }
   ],
   "source": [
    "def create_avro_dataset(records):\n",
    "    schema_path = schema_dir.joinpath('routes.avsc')\n",
    "    data_path = results_dir.joinpath('routes.avro')\n",
    "    ## TODO: Use fastavro to create Avro dataset\n",
    "    # schema = json.load(schema_path)\n",
    "    # parsed_schema = fastavro.schema.load_schema(schema_path)\n",
    "    with open(schema_path, 'r') as f:\n",
    "        parsed_schema = json.load(f)\n",
    "    \n",
    "    with open(data_path, 'wb') as out:\n",
    "        fastavro.writer(out, parsed_schema, records)\n",
    "\n",
    "    # used to test file output\n",
    "    # with open(data_path, 'rb') as fo:\n",
    "    #     avro_reader = fastavro.reader(fo)\n",
    "    #     for record in avro_reader:\n",
    "    #         print(record)\n",
    "        \n",
    "create_avro_dataset(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.c Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parquet_dataset():\n",
    "    src_data_path = '../../../data/processed/openflights/routes.jsonl.gz'\n",
    "    parquet_output_path = results_dir.joinpath('routes.parquet')\n",
    "    # s3 = s3fs.S3FileSystem(\n",
    "    #     anon=True,\n",
    "    #     client_kwargs={\n",
    "    #         'endpoint_url': endpoint_url\n",
    "    #     }\n",
    "    # )\n",
    "    \n",
    "    with open(src_data_path, 'rb') as f_gz:\n",
    "        with gzip.open(f_gz, 'rb') as f:\n",
    "            # pass\n",
    "            ## TODO: Use Apache Arrow to create Parquet table and save the dataset\n",
    "            record_data = pa.array([json.loads(line) for line in f.readlines()])\n",
    "    table = pa.Table.from_arrays([record_data], names=['Flight Info'])\n",
    "    pq.write_table(table, parquet_output_path)\n",
    "\n",
    "create_parquet_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.d Protocol Buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Parameter to MergeFrom() must be instance of same class: expected routes_pb2.Airport got NoneType.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 95\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(compressed_path, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     93\u001b[0m         f\u001b[39m.\u001b[39mwrite(snappy\u001b[39m.\u001b[39mcompress(routes\u001b[39m.\u001b[39mSerializeToString()))\n\u001b[1;32m---> 95\u001b[0m create_protobuf_dataset(records)\n",
      "Cell \u001b[1;32mIn[8], line 73\u001b[0m, in \u001b[0;36mcreate_protobuf_dataset\u001b[1;34m(records)\u001b[0m\n\u001b[0;32m     71\u001b[0m route\u001b[39m.\u001b[39mairline\u001b[39m.\u001b[39mCopyFrom(_airline_to_proto_obj(record[\u001b[39m\"\u001b[39m\u001b[39mairline\u001b[39m\u001b[39m\"\u001b[39m]))\n\u001b[0;32m     72\u001b[0m route\u001b[39m.\u001b[39msrc_airport\u001b[39m.\u001b[39mCopyFrom(_airport_to_proto_obj(record[\u001b[39m\"\u001b[39m\u001b[39msrc_airport\u001b[39m\u001b[39m\"\u001b[39m]))\n\u001b[1;32m---> 73\u001b[0m route\u001b[39m.\u001b[39;49mdst_airport\u001b[39m.\u001b[39;49mCopyFrom(_airport_to_proto_obj(record[\u001b[39m\"\u001b[39;49m\u001b[39mdst_airport\u001b[39;49m\u001b[39m\"\u001b[39;49m]))\n\u001b[0;32m     74\u001b[0m \u001b[39mif\u001b[39;00m record\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mcodeshare\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     75\u001b[0m     route\u001b[39m.\u001b[39mcodeshare \u001b[39m=\u001b[39m record[\u001b[39m\"\u001b[39m\u001b[39mcodeshare\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\google\\protobuf\\message.py:129\u001b[0m, in \u001b[0;36mMessage.CopyFrom\u001b[1;34m(self, other_msg)\u001b[0m\n\u001b[0;32m    127\u001b[0m   \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mClear()\n\u001b[1;32m--> 129\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mMergeFrom(other_msg)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py:1311\u001b[0m, in \u001b[0;36m_AddMergeFromMethod.<locals>.MergeFrom\u001b[1;34m(self, msg)\u001b[0m\n\u001b[0;32m   1309\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mMergeFrom\u001b[39m(\u001b[39mself\u001b[39m, msg):\n\u001b[0;32m   1310\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(msg, \u001b[39mcls\u001b[39m):\n\u001b[1;32m-> 1311\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   1312\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mParameter to MergeFrom() must be instance of same class: \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1313\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mexpected \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (_FullyQualifiedClassName(\u001b[39mcls\u001b[39m),\n\u001b[0;32m   1314\u001b[0m                                  _FullyQualifiedClassName(msg\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)))\n\u001b[0;32m   1316\u001b[0m   \u001b[39massert\u001b[39;00m msg \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[0;32m   1317\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Modified()\n",
      "\u001b[1;31mTypeError\u001b[0m: Parameter to MergeFrom() must be instance of same class: expected routes_pb2.Airport got NoneType."
     ]
    }
   ],
   "source": [
    "sys.path.insert(0, os.path.abspath('routes_pb2'))\n",
    "\n",
    "import routes_pb2\n",
    "\n",
    "def _airport_to_proto_obj(airport):\n",
    "    obj = routes_pb2.Airport()\n",
    "    if airport is None:\n",
    "        return None\n",
    "    if airport.get('airport_id') is None:\n",
    "        return None\n",
    "\n",
    "    obj.airport_id = airport.get('airport_id')\n",
    "    if airport.get('name'):\n",
    "        obj.name = airport.get('name')\n",
    "    if airport.get('city'):\n",
    "        obj.city = airport.get('city')\n",
    "    if airport.get('iata'):\n",
    "        obj.iata = airport.get('iata')\n",
    "    if airport.get('icao'):\n",
    "        obj.icao = airport.get('icao')\n",
    "    if airport.get('altitude'):\n",
    "        obj.altitude = airport.get('altitude')\n",
    "    if airport.get('timezone'):\n",
    "        obj.timezone = airport.get('timezone')\n",
    "    if airport.get('dst'):\n",
    "        obj.dst = airport.get('dst')\n",
    "    if airport.get('tz_id'):\n",
    "        obj.tz_id = airport.get('tz_id')\n",
    "    if airport.get('type'):\n",
    "        obj.type = airport.get('type')\n",
    "    if airport.get('source'):\n",
    "        obj.source = airport.get('source')\n",
    "\n",
    "    obj.latitude = airport.get('latitude')\n",
    "    obj.longitude = airport.get('longitude')\n",
    "\n",
    "    return obj\n",
    "\n",
    "\n",
    "def _airline_to_proto_obj(airline):\n",
    "    obj = routes_pb2.Airline()\n",
    "    ## TODO: Create an Airline obj using Protocol Buffers API\n",
    "    if airline is None:\n",
    "        return None\n",
    "    if airline.get('airline_id') is None:\n",
    "        return None\n",
    "    \n",
    "    obj.airline_id = airline.get('airline_id')\n",
    "    if airline.get('name'):\n",
    "        obj.name = airline.get('name')\n",
    "    if airline.get('city'):\n",
    "        obj.city = airline.get('alias')\n",
    "    if airline.get('iata'):\n",
    "        obj.iata = airline.get('iata')\n",
    "    if airline.get('icao'):\n",
    "        obj.icao = airline.get('icao')\n",
    "    if airline.get('altitude'):\n",
    "        obj.altitude = airline.get('callsign')\n",
    "    if airline.get('timezone'):\n",
    "        obj.timezone = airline.get('country')\n",
    "    if airline.get('dst'):\n",
    "        obj.dst = airline.get('active')\n",
    "    return obj\n",
    "\n",
    "\n",
    "def create_protobuf_dataset(records):\n",
    "    routes = routes_pb2.Routes()\n",
    "    for record in records:\n",
    "        route = routes_pb2.Route()\n",
    "        ## TODO: Implement the code to create the Protocol Buffers Dataset\n",
    "        route.airline.CopyFrom(_airline_to_proto_obj(record[\"airline\"]))\n",
    "        route.src_airport.CopyFrom(_airport_to_proto_obj(record[\"src_airport\"]))\n",
    "        route.dst_airport.CopyFrom(_airport_to_proto_obj(record[\"dst_airport\"]))\n",
    "        if record.get('codeshare'):\n",
    "            route.codeshare = record[\"codeshare\"]\n",
    "        if record.get('stops'):\n",
    "            route.stops = record[\"stops\"]\n",
    "        if record.get('equipment'):\n",
    "            route.equipment.append(\"equipment\")\n",
    "\n",
    "\n",
    "\n",
    "        routes.route.append(route)\n",
    "\n",
    "    data_path = results_dir.joinpath('routes.pb')\n",
    "\n",
    "    with open(data_path, 'wb') as f:\n",
    "        f.write(routes.SerializeToString())\n",
    "        \n",
    "    compressed_path = results_dir.joinpath('routes.pb.snappy')\n",
    "    \n",
    "    with open(compressed_path, 'wb') as f:\n",
    "        f.write(snappy.compress(routes.SerializeToString()))\n",
    "        \n",
    "create_protobuf_dataset(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.a Simple Geohash Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hash_dirs(records):\n",
    "    geoindex_dir = results_dir.joinpath('geoindex')\n",
    "    geoindex_dir.mkdir(exist_ok=True, parents=True)\n",
    "    hashes = []\n",
    "    ## TODO: Create hash index\n",
    "    for record in records:\n",
    "        if record.get('src_airport'):\n",
    "            hashes.append(pygeohash.encode(record[\"src_airport\"][\"longitude\"],record[\"src_airport\"][\"latitude\"]))\n",
    "        json_dir = f\"/results/geoindex/{hashes[-1][:1]}/{hashes[-1][:2]}/{hashes[-1][:3]}.jsonl.gz\"\n",
    "        cwd = os.getcwd()\n",
    "        os.makedirs(os.path.dirname(cwd+json_dir), exist_ok=True)\n",
    "\n",
    "        with gzip.open(cwd+json_dir, \"ab\") as f:\n",
    "            json_str = json.dumps(record).encode(\"utf-8\")\n",
    "            f.write(json_str + b\"\\n\")\n",
    "\n",
    "create_hash_dirs(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.b Simple Search Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9z5r677crc2j\n"
     ]
    }
   ],
   "source": [
    "def airport_search(latitude, longitude):\n",
    "    ## TODO: Create simple search to return nearest airport\n",
    "    src_loc = pygeohash.encode(latitude, longitude, precision=5)\n",
    "    dst_loc = pygeohash.encode(40.667489, -96.575328)\n",
    "    distance = pygeohash.geohash_approximate_distance(src_loc, dst_loc, check_validity=False)\n",
    "    print(dst_loc)\n",
    "    \n",
    "    \n",
    "airport_search(41.1499988, -95.91779)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
