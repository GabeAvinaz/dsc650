{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries and define common helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import fastavro\n",
    "import pygeohash\n",
    "import snappy \n",
    "import jsonschema\n",
    "from jsonschema.exceptions import ValidationError\n",
    "\n",
    "\n",
    "endpoint_url='https://storage.budsc.midwest-datascience.com'\n",
    "\n",
    "current_dir = Path(os.getcwd()).absolute()\n",
    "schema_dir = current_dir.joinpath('schemas')\n",
    "results_dir = current_dir.joinpath('results')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def read_jsonl_data():\n",
    "    # s3 = s3fs.S3FileSystem(\n",
    "    #     anon=True,\n",
    "    #     client_kwargs={\n",
    "    #         'endpoint_url': endpoint_url\n",
    "    #     }\n",
    "    # )\n",
    "    # src_data_path = 'data/processed/openflights/routes.jsonl.gz'\n",
    "    # with s3.open(src_data_path, 'rb') as f_gz:\n",
    "    #     with gzip.open(f_gz, 'rb') as f:\n",
    "    #         records = [json.loads(line) for line in f.readlines()]\n",
    "\n",
    "    src_data_path = '../../../data/processed/openflights/routes.jsonl.gz'\n",
    "    with gzip.open(src_data_path, 'rb') as f:\n",
    "        records = [json.loads(line) for line in f.readlines()]\n",
    "        \n",
    "\n",
    "    return records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the records from https://storage.budsc.midwest-datascience.com/data/processed/openflights/routes.jsonl.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = read_jsonl_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.a JSON Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_json_schema(records):\n",
    "    import genson\n",
    "\n",
    "    schema = genson.Schema()\n",
    "\n",
    "    for record in records:\n",
    "        schema.add_object(record)\n",
    "\n",
    "    json_schema = schema.to_dict()\n",
    "\n",
    "    schema_path = schema_dir.joinpath('routes-schema.json')\n",
    "    with open(schema_path, 'w') as f:\n",
    "        json.dump(json_schema, f, indent=2)\n",
    "\n",
    "def validate_jsonl_data(records):\n",
    "    schema_path = schema_dir.joinpath('routes-schema.json')\n",
    "\n",
    "    with open(schema_path) as f:\n",
    "        schema = json.load(f)\n",
    "\n",
    "    validation_csv_path = ('results/json_validation.md')\n",
    "    with open(validation_csv_path, 'w') as f:    \n",
    "        f.write(\"JSONL lines with failed validation\\n\")\n",
    "        f.write(\"----------------------------------\\n\")\n",
    "        for i, record in enumerate(records):\n",
    "            try:\n",
    "                jsonschema.validate(record, schema)\n",
    "            except ValidationError as e:\n",
    "                f.write(f\"Failed Entry: {i}\\n\")\n",
    "            \n",
    "# create_json_schema(records)\n",
    "validate_jsonl_data(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.b Avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_avro_dataset(records):\n",
    "    schema_path = schema_dir.joinpath('routes.avsc')\n",
    "    data_path = results_dir.joinpath('routes.avro')\n",
    "    ## TODO: Use fastavro to create Avro dataset\n",
    "    with open(schema_path, 'r') as f:\n",
    "        parsed_schema = json.load(f)\n",
    "    \n",
    "    with open(data_path, 'wb') as out:\n",
    "        fastavro.writer(out, parsed_schema, records)\n",
    "\n",
    "    # used to test file output\n",
    "    # with open(data_path, 'rb') as fo:\n",
    "    #     avro_reader = fastavro.reader(fo)\n",
    "    #     for record in avro_reader:\n",
    "    #         print(record)\n",
    "        \n",
    "create_avro_dataset(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.c Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parquet_dataset():\n",
    "    src_data_path = '../../../data/processed/openflights/routes.jsonl.gz'\n",
    "    parquet_output_path = results_dir.joinpath('routes.parquet')\n",
    "    # s3 = s3fs.S3FileSystem(\n",
    "    #     anon=True,\n",
    "    #     client_kwargs={\n",
    "    #         'endpoint_url': endpoint_url\n",
    "    #     }\n",
    "    # )\n",
    "    \n",
    "    with open(src_data_path, 'rb') as f_gz:\n",
    "        with gzip.open(f_gz, 'rb') as f:\n",
    "            # pass\n",
    "            ## TODO: Use Apache Arrow to create Parquet table and save the dataset\n",
    "            record_data = pa.array([json.loads(line) for line in f.readlines()])\n",
    "    table = pa.Table.from_arrays([record_data], names=['Flight Info'])\n",
    "    pq.write_table(table, parquet_output_path)\n",
    "\n",
    "create_parquet_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.d Protocol Buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.abspath('routes_pb2'))\n",
    "\n",
    "import routes_pb2\n",
    "\n",
    "def _airport_to_proto_obj(airport):\n",
    "    obj = routes_pb2.Airport()\n",
    "    if airport is None:\n",
    "        return None\n",
    "    if airport.get('airport_id') is None:\n",
    "        return None\n",
    "\n",
    "    obj.airport_id = airport.get('airport_id')\n",
    "    if airport.get('name'):\n",
    "        obj.name = airport.get('name')\n",
    "    if airport.get('city'):\n",
    "        obj.city = airport.get('city')\n",
    "    if airport.get('iata'):\n",
    "        obj.iata = airport.get('iata')\n",
    "    if airport.get('icao'):\n",
    "        obj.icao = airport.get('icao')\n",
    "    if airport.get('altitude'):\n",
    "        obj.altitude = airport.get('altitude')\n",
    "    if airport.get('timezone'):\n",
    "        obj.timezone = airport.get('timezone')\n",
    "    if airport.get('dst'):\n",
    "        obj.dst = airport.get('dst')\n",
    "    if airport.get('tz_id'):\n",
    "        obj.tz_id = airport.get('tz_id')\n",
    "    if airport.get('type'):\n",
    "        obj.type = airport.get('type')\n",
    "    if airport.get('source'):\n",
    "        obj.source = airport.get('source')\n",
    "\n",
    "    obj.latitude = airport.get('latitude')\n",
    "    obj.longitude = airport.get('longitude')\n",
    "\n",
    "    return obj\n",
    "\n",
    "\n",
    "def _airline_to_proto_obj(airline):\n",
    "    obj = routes_pb2.Airline()\n",
    "    ## TODO: Create an Airline obj using Protocol Buffers API\n",
    "    if airline is None:\n",
    "        return None\n",
    "    if airline.get('airline_id') is None:\n",
    "        return None\n",
    "    \n",
    "    obj.airline_id = airline.get('airline_id')\n",
    "    if airline.get('name'):\n",
    "        obj.name = airline.get('name')\n",
    "    if airline.get('city'):\n",
    "        obj.city = airline.get('alias')\n",
    "    if airline.get('iata'):\n",
    "        obj.iata = airline.get('iata')\n",
    "    if airline.get('icao'):\n",
    "        obj.icao = airline.get('icao')\n",
    "    if airline.get('altitude'):\n",
    "        obj.altitude = airline.get('callsign')\n",
    "    if airline.get('timezone'):\n",
    "        obj.timezone = airline.get('country')\n",
    "    if airline.get('active'):\n",
    "        obj.active = airline.get('active')\n",
    "    else:\n",
    "        obj.active = False\n",
    "    return obj\n",
    "\n",
    "\n",
    "def create_protobuf_dataset(records):\n",
    "    routes = routes_pb2.Routes()\n",
    "    for record in records:\n",
    "        route = routes_pb2.Route()\n",
    "        ## TODO: Implement the code to create the Protocol Buffers Dataset\n",
    "        route.airline.CopyFrom(_airline_to_proto_obj(record[\"airline\"]))\n",
    "        if record.get('src_airport'):\n",
    "            route.src_airport.CopyFrom(_airport_to_proto_obj(record[\"src_airport\"]))\n",
    "        if record.get('dst_airport'):\n",
    "            route.dst_airport.CopyFrom(_airport_to_proto_obj(record[\"dst_airport\"]))\n",
    "        # if record.get('codeshare'):\n",
    "        #     route.codeshare = record[\"codeshare\"]\n",
    "        if 'codeshare' in record and record['codeshare'] is not None:\n",
    "            route.codeshare = record['codeshare']\n",
    "        else:\n",
    "            route.codeshare = False\n",
    "\n",
    "        if record.get('stops'):\n",
    "            route.stops = record[\"stops\"]\n",
    "        if record.get('equipment'):\n",
    "            route.equipment.append(\"equipment\")\n",
    "\n",
    "\n",
    "\n",
    "        routes.route.append(route)\n",
    "\n",
    "    data_path = results_dir.joinpath('routes.pb')\n",
    "\n",
    "    with open(data_path, 'wb') as f:\n",
    "        f.write(routes.SerializeToString())\n",
    "        \n",
    "    compressed_path = results_dir.joinpath('routes.pb.snappy')\n",
    "    \n",
    "    with open(compressed_path, 'wb') as f:\n",
    "        f.write(snappy.compress(routes.SerializeToString()))\n",
    "        \n",
    "create_protobuf_dataset(records)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.e Output Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison results saved to: results/comparison.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gzip\n",
    "import pandas as pd\n",
    " \n",
    "\n",
    "def get_file_size(file_path):\n",
    "    \"\"\"Get the size of a file in bytes\"\"\"\n",
    "    return os.stat(file_path).st_size\n",
    "\n",
    "def get_gzip_size(filepath):\n",
    "    with open(filepath, 'rb') as f_in:\n",
    "        with gzip.open(filepath + \".gz\", 'wb') as f_out:\n",
    "            f_out.write(f_in.read())\n",
    "\n",
    "    size = os.stat(filepath + \".gz\").st_size\n",
    "    os.remove(filepath + \".gz\")\n",
    "    return size\n",
    "    \n",
    "\n",
    "\n",
    "def get_snappy_size(filepath):\n",
    "    if not os.path.isfile(filepath +'.snappy'):\n",
    "        with open(filepath, 'rb') as infile:\n",
    "            data = infile.read()\n",
    "            compressed_data = snappy.compress(data)\n",
    "            with open(filepath +'.snappy', 'wb') as outfile:\n",
    "                outfile.write(compressed_data)\n",
    "        size = os.stat(filepath + \".snappy\").st_size\n",
    "        os.remove(filepath + \".snappy\")\n",
    "        return size\n",
    "    return os.stat(filepath + \".snappy\").st_size\n",
    "\n",
    "# File paths\n",
    "avro_file = \"results/routes.avro\"\n",
    "pb_file = \"results/routes.pb\"\n",
    "json_file = \"results/json_validation.md\"\n",
    "parquet_file = \"results/routes.parquet\"\n",
    "\n",
    "output_file = \"results/comparison.csv\"\n",
    "\n",
    "\n",
    "entries = []\n",
    "entries.append({\"format\" : \"avro file\",\"uncompressed\" : get_file_size(avro_file),\"gzip\" : get_gzip_size(avro_file),\"snappy\" : get_snappy_size(avro_file)})\n",
    "entries.append({\"format\" : \"protocol buffer file\",\"uncompressed\" : get_file_size(pb_file),\"gzip\" : get_gzip_size(pb_file),\"snappy\" : get_snappy_size(pb_file)})\n",
    "entries.append({\"format\" : \"json Schema file\",\"uncompressed\" : get_file_size(json_file),\"gzip\" : get_gzip_size(json_file),\"snappy\" : get_snappy_size(json_file)})\n",
    "entries.append({\"format\" : \"parquet file\",\"uncompressed\" : get_file_size(parquet_file),\"gzip\" : get_gzip_size(parquet_file),\"snappy\" : get_snappy_size(parquet_file)})\n",
    "sizes_df = pd.DataFrame(entries)\n",
    "\n",
    "sizes_df.to_csv(output_file, sep=',')\n",
    "\n",
    "print(\"Comparison results saved to:\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.a Simple Geohash Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hash_dirs(records):\n",
    "    geoindex_dir = results_dir.joinpath('geoindex')\n",
    "    geoindex_dir.mkdir(exist_ok=True, parents=True)\n",
    "    hashes = []\n",
    "    ## TODO: Create hash index\n",
    "    for record in records:\n",
    "        if record.get('src_airport'):\n",
    "            hashes.append(pygeohash.encode(record[\"src_airport\"][\"longitude\"],record[\"src_airport\"][\"latitude\"]))\n",
    "        json_dir = f\"/results/geoindex/{hashes[-1][:1]}/{hashes[-1][:2]}/{hashes[-1][:3]}.jsonl.gz\"\n",
    "        cwd = os.getcwd()\n",
    "        os.makedirs(os.path.dirname(cwd+json_dir), exist_ok=True)\n",
    "\n",
    "        with gzip.open(cwd+json_dir, \"ab\") as f:\n",
    "            json_str = json.dumps(record).encode(\"utf-8\")\n",
    "            f.write(json_str + b\"\\n\")\n",
    "\n",
    "create_hash_dirs(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.b Simple Search Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest airport is :  Eppley Airfield\n",
      "Distance of:  19.545 Km\n"
     ]
    }
   ],
   "source": [
    "def airport_search(latitude, longitude):\n",
    "    ## TODO: Create simple search to return nearest airport\n",
    "    src_loc = pygeohash.encode(latitude, longitude, precision=5)\n",
    "    records = read_jsonl_data()\n",
    "    short_distance = 20000000\n",
    "    short_record = {}\n",
    "    for record in records:\n",
    "        if record.get('src_airport'):\n",
    "            temp_loc = pygeohash.encode(record[\"src_airport\"][\"latitude\"], record[\"src_airport\"][\"longitude\"], precision=5)\n",
    "            if pygeohash.geohash_approximate_distance(src_loc, temp_loc, check_validity=False) < short_distance:\n",
    "                short_distance = pygeohash.geohash_approximate_distance(src_loc, temp_loc, check_validity=False)\n",
    "                short_record = record\n",
    "\n",
    "    print(f\"Closest airport is :  {short_record['src_airport']['name']}\")\n",
    "    print(f\"Distance of:  {short_distance/1000} Km\")\n",
    "    \n",
    "# airport_search(48.04261750114304, 2.8295283335661248) \n",
    "airport_search(41.1499988, -95.91779)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
